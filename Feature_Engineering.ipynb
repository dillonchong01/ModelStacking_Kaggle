{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc9bd6a6",
      "metadata": {
        "id": "fc9bd6a6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from itertools import combinations\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35be745a",
      "metadata": {
        "id": "35be745a"
      },
      "source": [
        "## Data Preprocessing and Basic Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59821e3a",
      "metadata": {
        "id": "59821e3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: (593994, 135)\n",
            "Test data shape: (254569, 134)\n",
            "Number of features created: 121\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "train_df = pd.read_csv(r\"C:\\Users\\dillo\\Downloads\\train.csv\")\n",
        "test_df = pd.read_csv(r\"C:\\Users\\dillo\\Downloads\\test.csv\")\n",
        "test_ids = test_df['id'].copy()\n",
        "\n",
        "# Configuration\n",
        "target = 'loan_paid_back'\n",
        "categories = [\n",
        "    'gender', 'marital_status', 'education_level', 'employment_status',\n",
        "    'loan_purpose', 'grade_subgrade', 'annual_income_bin', 'loan_amount_bin',\n",
        "    'debt_to_income_ratio', 'credit_score', 'interest_rate'\n",
        "]\n",
        "SMOOTHING = 100\n",
        "N_SPLITS = 5\n",
        "N_BINS = 250\n",
        "\n",
        "# Quantile Binning on Training Data\n",
        "train_df['annual_income_bin'], income_bins = pd.qcut(\n",
        "    train_df['annual_income'], N_BINS, labels=False, duplicates='drop', retbins=True\n",
        ")\n",
        "train_df['loan_amount_bin'], loan_bins = pd.qcut(\n",
        "    train_df['loan_amount'], N_BINS, labels=False, duplicates='drop', retbins=True\n",
        ")\n",
        "\n",
        "# Apply Bins to Test Data\n",
        "test_df['annual_income_bin'] = pd.cut(\n",
        "    test_df['annual_income'], bins=income_bins, labels=False, include_lowest=True\n",
        ")\n",
        "test_df['loan_amount_bin'] = pd.cut(\n",
        "    test_df['loan_amount'], bins=loan_bins, labels=False, include_lowest=True\n",
        ")\n",
        "\n",
        "# Handle Out-Of-Range Values\n",
        "test_df['annual_income_bin'] = test_df['annual_income_bin'].fillna(-1).astype(int)\n",
        "test_df['loan_amount_bin'] = test_df['loan_amount_bin'].fillna(-1).astype(int)\n",
        "\n",
        "# K-Fold Setup\n",
        "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
        "global_mean = train_df[target].mean()\n",
        "pair_cols = list(combinations(categories, 2))\n",
        "\n",
        "single_te_cols = [f'{col}_te' for col in categories]\n",
        "pair_te_cols = [f'{col1}_{col2}_te' for col1, col2 in pair_cols]\n",
        "pair_freq_cols = [f'{col1}_{col2}_freq' for col1, col2 in pair_cols]\n",
        "\n",
        "# Pre-allocate Empty Numpy Arrays for New Features\n",
        "n_train = len(train_df)\n",
        "n_test = len(test_df)\n",
        "\n",
        "train_single_te = np.zeros((n_train, len(categories)), dtype=np.float32)\n",
        "train_pair_te = np.zeros((n_train, len(pair_cols)), dtype=np.float32)\n",
        "train_pair_freq = np.zeros((n_train, len(pair_cols)), dtype=np.float32)\n",
        "\n",
        "test_single_te = np.zeros((n_test, len(categories)), dtype=np.float32)\n",
        "test_pair_te = np.zeros((n_test, len(pair_cols)), dtype=np.float32)\n",
        "test_pair_freq = np.zeros((n_test, len(pair_cols)), dtype=np.float32)\n",
        "\n",
        "# K-Fold Feature Engineering for Train Data\n",
        "for train_idx, val_idx in kf.split(train_df):\n",
        "    train_fold = train_df.iloc[train_idx]\n",
        "    val_fold = train_df.iloc[val_idx]\n",
        "    \n",
        "    # Single-Column Target Encoding with Smoothing\n",
        "    for i, col in enumerate(categories):\n",
        "        agg = train_fold.groupby(col, observed=False)[target].agg(['sum', 'count'])\n",
        "        smoothed_means = (agg['sum'] + global_mean * SMOOTHING) / (agg['count'] + SMOOTHING)\n",
        "        train_single_te[val_idx, i] = val_fold[col].map(smoothed_means).fillna(global_mean).values\n",
        "    \n",
        "    # Pairwise Target & Frequency Encoding\n",
        "    for j, (col1, col2) in enumerate(pair_cols):\n",
        "        # Group by multiple columns directly\n",
        "        grouped = train_fold.groupby([col1, col2], observed=False)\n",
        "        \n",
        "        # Target Encoding with Smoothing\n",
        "        agg = grouped[target].agg(['sum', 'count'])\n",
        "        smoothed_means = (agg['sum'] + global_mean * SMOOTHING) / (agg['count'] + SMOOTHING)\n",
        "        \n",
        "        # Map to Validation Fold using MultiIndex\n",
        "        val_index = pd.MultiIndex.from_arrays([val_fold[col1].values, val_fold[col2].values])\n",
        "        train_pair_te[val_idx, j] = smoothed_means.reindex(val_index, fill_value=global_mean).values\n",
        "        \n",
        "        # Frequency Encoding\n",
        "        freq_counts = grouped.size()\n",
        "        train_pair_freq[val_idx, j] = freq_counts.reindex(val_index, fill_value=0).values\n",
        "\n",
        "# Feature Engineering for Test Data (using full training data)\n",
        "# Single-Column Target Encoding with Smoothing\n",
        "for i, col in enumerate(categories):\n",
        "    agg = train_df.groupby(col, observed=False)[target].agg(['sum', 'count'])\n",
        "    smoothed_means = (agg['sum'] + global_mean * SMOOTHING) / (agg['count'] + SMOOTHING)\n",
        "    test_single_te[:, i] = test_df[col].map(smoothed_means).fillna(global_mean).values\n",
        "\n",
        "# Pairwise Target & Frequency Encoding\n",
        "for j, (col1, col2) in enumerate(pair_cols):\n",
        "    grouped = train_df.groupby([col1, col2], observed=False)\n",
        "    \n",
        "    # Target Encoding with Smoothing\n",
        "    agg = grouped[target].agg(['sum', 'count'])\n",
        "    smoothed_means = (agg['sum'] + global_mean * SMOOTHING) / (agg['count'] + SMOOTHING)\n",
        "    \n",
        "    # Map to Test Set using MultiIndex\n",
        "    test_index = pd.MultiIndex.from_arrays([test_df[col1].values, test_df[col2].values])\n",
        "    test_pair_te[:, j] = smoothed_means.reindex(test_index, fill_value=global_mean).values\n",
        "    \n",
        "    # Frequency Encoding\n",
        "    freq_counts = grouped.size()\n",
        "    test_pair_freq[:, j] = freq_counts.reindex(test_index, fill_value=0).values\n",
        "\n",
        "# Convert arrays to DataFrames\n",
        "train_single_te_df = pd.DataFrame(train_single_te, columns=single_te_cols, index=train_df.index)\n",
        "train_pair_te_df = pd.DataFrame(train_pair_te, columns=pair_te_cols, index=train_df.index)\n",
        "train_pair_freq_df = pd.DataFrame(train_pair_freq, columns=pair_freq_cols, index=train_df.index)\n",
        "\n",
        "test_single_te_df = pd.DataFrame(test_single_te, columns=single_te_cols, index=test_df.index)\n",
        "test_pair_te_df = pd.DataFrame(test_pair_te, columns=pair_te_cols, index=test_df.index)\n",
        "test_pair_freq_df = pd.DataFrame(test_pair_freq, columns=pair_freq_cols, index=test_df.index)\n",
        "\n",
        "# Concatenate all features at once\n",
        "train_df = pd.concat([train_df, train_single_te_df, train_pair_te_df, train_pair_freq_df], axis=1)\n",
        "test_df = pd.concat([test_df, test_single_te_df, test_pair_te_df, test_pair_freq_df], axis=1)\n",
        "\n",
        "train_df.drop(columns=['id'], inplace=True)\n",
        "test_df.drop(columns=['id'], inplace=True)\n",
        "\n",
        "# Convert Categorical Columns to 'category' dtype\n",
        "for col in categories:\n",
        "    train_df[col] = train_df[col].astype('category')\n",
        "    test_df[col] = test_df[col].astype('category')\n",
        "\n",
        "numeric = [col for col in train_df.columns if col not in categories + [target]]\n",
        "\n",
        "print(f\"Training data shape: {train_df.shape}\")\n",
        "print(f\"Test data shape: {test_df.shape}\")\n",
        "print(f\"Number of features created: {len(single_te_cols) + len(pair_te_cols) + len(pair_freq_cols)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6847d6a",
      "metadata": {},
      "source": [
        "## **Function to Get Feature Importance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdd8785c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_feature_importances(train_df, target, numerical, categorical, random_state=RANDOM_SEED):\n",
        "    \"\"\"\n",
        "    Train a quick LightGBM model and return feature importances\n",
        "    \"\"\"\n",
        "    df = train_df.copy()\n",
        "    for col in categorical:\n",
        "        df[col] = train_df[col].astype('category')\n",
        "    \n",
        "    features = numerical + categorical\n",
        "    X = df[features]\n",
        "    y = df[target]\n",
        "    \n",
        "    # Train LightGBM\n",
        "    model = lgb.LGBMRegressor(n_estimators=1000, random_state=random_state, verbose=-1)\n",
        "    model.fit(X, y, categorical_feature=categorical)\n",
        "    \n",
        "    # Get Feature Importance\n",
        "    importances = pd.Series(model.feature_importances_, index=features)\n",
        "    importances = importances.sort_values(ascending=False)\n",
        "    \n",
        "    print(\"Feature importances:\")\n",
        "    for feat, imp in importances.items():\n",
        "        print(f\"{feat}: {imp}\")\n",
        "    \n",
        "    return importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n1OgUcsWvAM4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1OgUcsWvAM4",
        "outputId": "7b39d844-0d2d-4aff-81be-6bea66d3feb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drop Columns:\n",
            "['gender_grade_subgrade_freq', 'credit_score_interest_rate_freq', 'loan_amount_bin_te', 'gender_marital_status_freq', 'annual_income_bin_loan_amount_bin_freq', 'debt_to_income_ratio_interest_rate_freq', 'gender_loan_purpose_freq', 'grade_subgrade_loan_amount_bin_freq', 'loan_purpose_te', 'marital_status_te', 'annual_income_bin_interest_rate_freq', 'education_level_grade_subgrade_freq', 'gender_education_level_te', 'marital_status_education_level_freq', 'loan_amount_bin_interest_rate_freq', 'loan_amount_bin_credit_score_freq', 'loan_purpose', 'education_level_te', 'gender_te', 'education_level', 'gender', 'marital_status']\n",
            "Numeric Columns:\n",
            "['annual_income', 'loan_amount', 'employment_status_te', 'annual_income_bin_te', 'debt_to_income_ratio_te', 'credit_score_te', 'interest_rate_te', 'gender_marital_status_te', 'gender_employment_status_te', 'gender_grade_subgrade_te', 'gender_annual_income_bin_te', 'gender_debt_to_income_ratio_te', 'gender_credit_score_te', 'gender_interest_rate_te', 'marital_status_education_level_te', 'marital_status_employment_status_te', 'marital_status_loan_purpose_te', 'marital_status_grade_subgrade_te', 'marital_status_annual_income_bin_te', 'marital_status_debt_to_income_ratio_te', 'marital_status_credit_score_te', 'marital_status_interest_rate_te', 'education_level_employment_status_te', 'education_level_loan_purpose_te', 'education_level_grade_subgrade_te', 'education_level_annual_income_bin_te', 'education_level_loan_amount_bin_te', 'education_level_debt_to_income_ratio_te', 'education_level_credit_score_te', 'education_level_interest_rate_te', 'employment_status_loan_purpose_te', 'employment_status_grade_subgrade_te', 'employment_status_annual_income_bin_te', 'employment_status_loan_amount_bin_te', 'employment_status_debt_to_income_ratio_te', 'employment_status_credit_score_te', 'employment_status_interest_rate_te', 'loan_purpose_grade_subgrade_te', 'loan_purpose_annual_income_bin_te', 'loan_purpose_loan_amount_bin_te', 'loan_purpose_debt_to_income_ratio_te', 'loan_purpose_credit_score_te', 'loan_purpose_interest_rate_te', 'grade_subgrade_annual_income_bin_te', 'grade_subgrade_loan_amount_bin_te', 'grade_subgrade_debt_to_income_ratio_te', 'grade_subgrade_credit_score_te', 'grade_subgrade_interest_rate_te', 'annual_income_bin_loan_amount_bin_te', 'annual_income_bin_debt_to_income_ratio_te', 'annual_income_bin_credit_score_te', 'annual_income_bin_interest_rate_te', 'loan_amount_bin_debt_to_income_ratio_te', 'loan_amount_bin_credit_score_te', 'loan_amount_bin_interest_rate_te', 'debt_to_income_ratio_credit_score_te', 'debt_to_income_ratio_interest_rate_te', 'credit_score_interest_rate_te', 'gender_employment_status_freq', 'gender_annual_income_bin_freq', 'gender_loan_amount_bin_freq', 'gender_debt_to_income_ratio_freq', 'gender_interest_rate_freq', 'marital_status_employment_status_freq', 'marital_status_annual_income_bin_freq', 'marital_status_loan_amount_bin_freq', 'marital_status_debt_to_income_ratio_freq', 'marital_status_credit_score_freq', 'marital_status_interest_rate_freq', 'education_level_employment_status_freq', 'education_level_annual_income_bin_freq', 'education_level_debt_to_income_ratio_freq', 'education_level_credit_score_freq', 'education_level_interest_rate_freq', 'employment_status_loan_purpose_freq', 'employment_status_grade_subgrade_freq', 'employment_status_annual_income_bin_freq', 'employment_status_loan_amount_bin_freq', 'employment_status_debt_to_income_ratio_freq', 'employment_status_credit_score_freq', 'employment_status_interest_rate_freq', 'loan_purpose_grade_subgrade_freq', 'loan_purpose_annual_income_bin_freq', 'loan_purpose_loan_amount_bin_freq', 'loan_purpose_debt_to_income_ratio_freq', 'loan_purpose_credit_score_freq', 'grade_subgrade_annual_income_bin_freq', 'grade_subgrade_debt_to_income_ratio_freq', 'annual_income_bin_debt_to_income_ratio_freq', 'annual_income_bin_credit_score_freq', 'loan_amount_bin_debt_to_income_ratio_freq', 'debt_to_income_ratio_credit_score_freq']\n",
            "Categorical Columns:\n",
            "['employment_status', 'grade_subgrade', 'annual_income_bin', 'loan_amount_bin', 'debt_to_income_ratio', 'credit_score', 'interest_rate']\n"
          ]
        }
      ],
      "source": [
        "# Get Feature Importance\n",
        "importances = get_feature_importances(train_df, target, numeric, categories)\n",
        "\n",
        "# Remove Columns where Importance <= 10\n",
        "zero_feats = list(importances[importances <= 15].index)\n",
        "print(\"Drop Columns:\")\n",
        "print(zero_feats)\n",
        "\n",
        "numeric = [col for col in numeric if col not in zero_feats]\n",
        "categories = [col for col in categories if col not in zero_feats]\n",
        "print(\"Numeric Columns:\")\n",
        "print(numeric)\n",
        "print(\"Categorical Columns:\")\n",
        "print(categories)\n",
        "\n",
        "train_df = train_df.drop(columns=zero_feats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d5f291b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training LightGBM...\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "[100]\tvalid_0's auc: 0.918603\n",
            "[200]\tvalid_0's auc: 0.920814\n",
            "[300]\tvalid_0's auc: 0.922464\n",
            "[400]\tvalid_0's auc: 0.923224\n",
            "[500]\tvalid_0's auc: 0.923465\n",
            "[600]\tvalid_0's auc: 0.923537\n",
            "Early stopping, best iteration is:\n",
            "[617]\tvalid_0's auc: 0.923545\n",
            "\n",
            "Test AUC: 0.9235\n",
            "Best iteration: 617\n",
            "Total features used: 112\n"
          ]
        }
      ],
      "source": [
        "# Test AUC of LightGBM\n",
        "X = train_df.drop(columns=[target])\n",
        "y = train_df[target]\n",
        "\n",
        "# 70-30 split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_SEED, stratify=y)\n",
        "\n",
        "# Train LightGBM with categorical features specified\n",
        "print(\"\\nTraining LightGBM...\")\n",
        "lgb_train = lgb.Dataset(X_train, y_train, categorical_feature=categories)\n",
        "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train, categorical_feature=categories)\n",
        "\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'auc',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'learning_rate': 0.01,\n",
        "    'num_leaves': 31,\n",
        "    'random_state': RANDOM_SEED\n",
        "}\n",
        "\n",
        "model = lgb.train(params, lgb_train, num_boost_round=10000, valid_sets=[lgb_eval], callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)])\n",
        "\n",
        "# Predict and calculate AUC\n",
        "y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
        "auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nTest AUC: {auc:.4f}\")\n",
        "print(f\"Best iteration: {model.best_iteration}\")\n",
        "print(f\"Total features used: {len(X.columns)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "193d318f",
      "metadata": {},
      "outputs": [],
      "source": [
        "drop_cols = ['gender_grade_subgrade_freq', 'credit_score_interest_rate_freq',\n",
        "             'loan_amount_bin_te', 'gender_marital_status_freq', 'annual_income_bin_loan_amount_bin_freq',\n",
        "             'debt_to_income_ratio_interest_rate_freq', 'gender_loan_purpose_freq',\n",
        "             'grade_subgrade_loan_amount_bin_freq', 'loan_purpose_te', 'marital_status_te',\n",
        "             'annual_income_bin_interest_rate_freq', 'education_level_grade_subgrade_freq',\n",
        "             'gender_education_level_te', 'marital_status_education_level_freq',\n",
        "             'loan_amount_bin_interest_rate_freq', 'loan_amount_bin_credit_score_freq', 'loan_purpose',\n",
        "             'education_level_te', 'gender_te', 'education_level', 'gender', 'marital_status']"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
